[{"authors":["admin"],"categories":null,"content":"I’m a PhD student at King Abdullah University of Science and Technology (KAUST), pursuing to advance artificial intelligence via understanding deep neural networks. I am part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem. I received my MSc degree in Computer Science from KAUST, and my undergraduate degree form King Fahd University of Petroleum and Minerals (KFUPM) in Software Engineering. I am a curious scholar with an appetite for learning. I want to understand the world around me and be at the forefront of the unknown. Fully dedicated to fulfilling my duty of passing on knowledge to future generations.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://modar.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I’m a PhD student at King Abdullah University of Science and Technology (KAUST), pursuing to advance artificial intelligence via understanding deep neural networks. I am part of the Image and Video Understanding Laboratory (IVUL) in the Visual Computing Center (VCC), advised by Bernard Ghanem. I received my MSc degree in Computer Science from KAUST, and my undergraduate degree form King Fahd University of Petroleum and Minerals (KFUPM) in Software Engineering. I am a curious scholar with an appetite for learning.","tags":null,"title":"Modar Alfadly","type":"authors"},{"authors":["Jia-Hong Huang","Cuong Dao-Duc\u0026ast;","**Modar Alfadly**\u0026ast;","Bernard Ghanem"],"categories":null,"content":"In AAAI19, a record number of over 7700 papers were submitted, 7095 were reviewed, a record low of 1150 papers (16.2%) were accepted [source1 - sourc2]\n","date":1536094800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536094800,"objectID":"41264d4830989d7f6b22e41e679b8124","permalink":"https://modar.me/publication/vqabq-aaai19/","publishdate":"2018-09-05T00:00:00+03:00","relpermalink":"/publication/vqabq-aaai19/","section":"publication","summary":"Deep neural networks have been playing an essential role in many computer vision tasks including Visual Question Answering (VQA). Until recently, the study of their accuracy was the main focus of research but now there is a trend toward assessing the robustness of these models against adversarial attacks by evaluating their tolerance to varying noise levels. In VQA, adversarial attacks can target the image and/or the proposed main question and yet there is a lack of proper analysis of the later. In this work, we propose a flexible framework that focuses on the language part of VQA that uses semantically relevant questions, dubbed basic questions, acting as controllable noise to evaluate the robustness of VQA models. We hypothesize that the level of noise is positively correlated to the similarity of a basic question to the main question. Hence, to apply noise on any given main question, we rank a pool of basic questions based on their similarity by casting this ranking task as a LASSO optimization problem. Then, we propose a novel robustness measure Rscore and two large-scale basic question datasets (BQDs) in order to standardize robustness analysis for VQA models.","tags":["vqa","robustness"],"title":"A Novel Framework for Robustness Analysis of Visual QA Models","type":"publication"},{"authors":["Jia-Hong Huang","**Modar Alfadly**","Bernard Ghanem"],"categories":null,"content":"This paper was accepted to the Visual Dialog Workshop in CVPR18 [source]\n","date":1527195600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527195600,"objectID":"b902d681ff7d500a38ab921ab699fbe0","permalink":"https://modar.me/publication/vqabq-cvpr18/","publishdate":"2018-05-25T00:00:00+03:00","relpermalink":"/publication/vqabq-cvpr18/","section":"publication","summary":"Visual Question Answering (VQA) models should have both high robustness and accuracy. Unfortunately, most of the current VQA research only focuses on accuracy because there is a lack of proper methods to measure the robustness of VQA models. There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the ranked basic questions, with similarity scores, of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question about the given image. We claim that a robust VQA model is one, whose performance is not changed much when related basic questions as also made available to it as input. We formulate the basic questions generation problem as a LASSO optimization, and also propose a large scale Basic Question Dataset (BQD) and Rscore (novel robustness measure), for analyzing the robustness of VQA models. We hope our BQD will be used as a benchmark for to evaluate the robustness of VQA models, so as to help the community build more robust and accurate VQA models.","tags":["vqa"],"title":"Robustness Analysis of Visual QA Models by Basic Questions","type":"publication"},{"authors":["Adel Bibi\u0026ast;","**Modar Alfadly**\u0026ast;","Bernard Ghanem"],"categories":null,"content":"In CVPR18, a total of 3359 papers were submitted, 979 papers (29.1%) were accepted, 224 were Spotlights (6.7%), and 70 were Orals (2.1%) [source]\n","date":1510693200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510693200,"objectID":"0e95e4dc5afdb64476600f04adca17a9","permalink":"https://modar.me/publication/gnm-cvpr18/","publishdate":"2017-11-15T00:00:00+03:00","relpermalink":"/publication/gnm-cvpr18/","section":"publication","summary":"The outstanding performance of deep neural networks (DNNs), for the visual recognition task in particular, has been demonstrated on several large-scale benchmarks. This performance has immensely strengthened the line of research that aims to understand and analyze the driving reasons behind the effectiveness of these networks. One important aspect of this analysis has recently gained much attention, namely the reaction of a DNN to noisy input. This has spawned research on developing adversarial input attacks as well as training strategies that make DNNs more robust against these attacks. To this end, we derive in this paper exact analytic expressions for the first and second moments (mean and variance) of a small piecewise linear (PL) network (Affine, ReLU, Affine) subject to general Gaussian input. We experimentally show that these expressions are tight under simple linearizations of deeper PL-DNNs, especially popular architectures in the literature (e.g. LeNet and AlexNet). Extensive experiments on image classification show that these expressions can be used to study the behaviour of the output mean of the logits for each class, the interclass confusion and the pixel-level spatial noise sensitivity of the network. Moreover, we show how these expressions can be used to systematically construct targeted and non-targeted adversarial attacks.","tags":["network moments"],"title":"Analytic Expressions for Probabilistic Moments of PL-DNN with Gaussian Input","type":"publication"},{"authors":["Jia-Hong Huang","**Modar Alfadly**","Bernard Ghanem"],"categories":null,"content":"This paper was accepted to the 2nd VQA workshop in CVPR17 [source]\n","date":1495918800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495918800,"objectID":"0b4758dc93cb83eb5a8f94239256f26c","permalink":"https://modar.me/publication/vqabq-cvpr17/","publishdate":"2017-05-28T00:00:00+03:00","relpermalink":"/publication/vqabq-cvpr17/","section":"publication","summary":"Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.","tags":["vqa"],"title":"VQABQ: Visual Question Answering by Basic Questions","type":"publication"}]